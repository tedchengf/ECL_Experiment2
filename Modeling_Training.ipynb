{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Training Notebook\n",
    "<hr>\n",
    "\n",
    "This is the Jupyter Notebook to get you started on writing a simple Bayesian\n",
    "Model. To complete this, you need basic understanding of programming in python\n",
    "and of Bayesian inference. If you run into problems, you should first try do you\n",
    "research before asking me for solutions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Overview\n",
    "The task for your model is identitcal to the task we've given to the subjects.\n",
    "That is, given a series of combinations of objects, what is the most probable deterministic\n",
    "rule that govern the outcomes (explosion or no explosion) of all these\n",
    "combinations? \n",
    "\n",
    "There are several key details you should know before starting. First, there are\n",
    "three binary features: \n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{Color} &=& \\{\\text{Red, Blue}\\} \\\\\n",
    "\\text{Shape} &=& \\{\\text{Circle, Square}\\} \\\\\n",
    "\\text{Size}  &=& \\{\\text{Large, Small}\\} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "This yields $2^3 = 8$ unique objects. Next, these objects are arranged in a\n",
    "2-object *multiset combination*, that is to say, objects can be repeated in this\n",
    "combination. As a result, there are $nCr(8+2-1, 2) = 36$ unique multiset\n",
    "combinations (if you are curious, this is calculated through *multinomial\n",
    "coefficients*, which is equivalent to dipositing $8$ distinct objects into $2$\n",
    "distinct bins).\n",
    "\n",
    "A set of $36$ 2-object combination seems small, but the rule learning problem that stems\n",
    "from this set is immense. To give you a sense of the complexity at hand, we can\n",
    "consider the number of possible *extensions* of this rule. Since we are working\n",
    "with boolean outcome (a combination will either trigger the effect or not), the\n",
    "extension of a rule is a partition of the $36$ item into two *complementary*\n",
    "sets. Allowing empty set, this will give us $2^{36} = 6.87194767\\times10^{10}$\n",
    "unique partitions (and hence unique rules). To be clear, this is an extremely large hypothesis space in\n",
    "the context of higher cognition, and human learners probably only test a tiny\n",
    "fraction of these rules. It is our job to figure what what are the rules that\n",
    "makes up this small fraction, and why they decide on these rules instead of\n",
    "other.\n",
    "\n",
    "A common way to approach this question is to investigate these rules from the\n",
    "*intension* perspective. For example, we can represent the rules as first-order\n",
    "logic expressions. Instead of investigating all the $6.87194767\\times10^{10}$ of\n",
    "unique rules, we can instead focus on several *families* of rules that have\n",
    "specific forms with the assumption that only these families are readily\n",
    "avaliable to the learners. In this task, you will implement two of the rule families\n",
    "we've used. The *type-singular* family specifies the feature combinations within an\n",
    "obect boundary; the *counting conjunction* family specifies the overall frequency of\n",
    "features within the 2-object combination regardless of the object boundary. We will get into the details of these rules later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "The model is split into two parts: object/rule representation and bayesian modeling. The\n",
    "representation part deals with representing the stimuli, generating rules\n",
    "that fill out a hypotheses space, and verifying whether a given stimuli obeys a\n",
    "given rule. The bayesian part manages the probablistic distribution (*posterior*)\n",
    "over all rules in the hypotheses space, and updates this distribution whenever a\n",
    "new stimuli is encountered. You will first implement the rule representation\n",
    "part before writing out the bayesian model.\n",
    "\n",
    "Some advices before you jump in: \n",
    "1. Think carefully about how you represent the rules and the stimuli. This will\n",
    "   be the core part of your model and will be the hardest one to change.\n",
    "2. When you implement a model, you should always keep in mind that you might\n",
    "   need to modify your model in the future (e.g., you need to accomadate\n",
    "   different rules or need a more advanced bayesian model). Don't engineer\n",
    "   yourself into a corner.\n",
    "3. Avoid premature optimization. This might not be applicable to you, but I\n",
    "   usually get distracted by trying to write the most efficient algorithm for a\n",
    "   small task at the expense of modularity and readability. You should only\n",
    "   optimize your code if some operations eat up a huge chunk of your runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from itertools import product, combinations, permutations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are some basic libraries you will need to get your model to work, but feel\n",
    "free to import more. Note, however, you should not import readily made models for\n",
    "bayesian modeling.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Internal Representation\n",
    "The core of your model is the representations (in terms of data structure) it\n",
    "use. There are two important parts: object representations encode the stimuli,\n",
    "and the rule representations encode the rules you give to your model. Keep in\n",
    "mind that the two representations must be able to recognize each other. That is,\n",
    "you must design these two systems of representation in a way that the rules must\n",
    "be able to determine whether a given stimulus satisfy itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Object Representation\n",
    "Recall that we have $8$ unique objects that varies in $3$ binary dimensions.\n",
    "Each training stimuli comes in with $2$ objects, and each generalization stimuli\n",
    "come with $3$. \n",
    "\n",
    "Along with this notebook there is also the `all_data.xslx` file, which encode\n",
    "all the 211 stimuli a subject have seen. You will find the detailed information\n",
    "in the excel file, but the object coding is shown below:\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "2 &:& \\text{Red, Circle, Large} \\\\\n",
    "3 &:& \\text{Red, Circle, Small} \\\\\n",
    "5 &:& \\text{Red, Triangle, Large} \\\\\n",
    "7 &:& \\text{Red, Triangle, Small} \\\\\n",
    "11 &:& \\text{Blue, Circle, Large} \\\\\n",
    "13 &:& \\text{Blue, Circle, Small} \\\\\n",
    "17 &:& \\text{Blue, Triangle, Large} \\\\\n",
    "19 &:& \\text{Blue, Triangle, Small} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "Each stimulus comes in as a sequence. For example, a sequence of $[2,3]$ means\n",
    "the stimulus is a red large circle on the left and a red small circle on the\n",
    "right. As a side note, you might notice that the object codings are all in prime number, and\n",
    "this is intentional. In my implementation, I represented object with\n",
    "prime number as a simplifed form of Godel encoding, since doing so will enforce\n",
    "the products of every sets of objects to be unique. Checking whether two\n",
    "sequences are the same combinations boils down to checking if they have the same\n",
    "product. This sounds like a clever design, but it's practically inferior (unless\n",
    "you use Godel numbering to code individual features and somehow managed to\n",
    "exploit those prime numbers in rule verification) since it's slow (not in\n",
    "checking products, but in situations when you need to do prime factorization) and unreadible\n",
    "without a table. I advice you to come up with your own way of representing\n",
    "objects, but feel free to exploit Godel encoding if you so desire.\n",
    "\n",
    "You have two specific tasks:\n",
    "1. **You should device a representation scheme for the stimuli that encodes the\n",
    "   three features of each object and preserve the order the objects in a\n",
    "   stimulus.** Although the rule we test do not care about this order, we still\n",
    "   need to keep this information in case we want to take it into account in the\n",
    "   future.\n",
    "2. **You should write some code to read the subject's data in the excel file and\n",
    "  recast all the stimuli in your own object representation**. You don't need to\n",
    "  read the excel file directly; you can, for example, just copy the subjects' data\n",
    "  into a txt file or the csv file and read them with numpy or pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Type-Singular Rule Representation\n",
    "Now that you have created a representation scheme for objects, you can move to\n",
    "create a representation schemes for the rules. The rule space you will implement\n",
    "is the object-oriented rule space. Generally speaking, an object-oriented rule\n",
    "take the form of the following:\n",
    "$$\n",
    "\\exists_x[\\text{Color(x) = Red} \\land \\text{Shape(x) = Circle} \\land\n",
    "\\text{Size(x) = Large}]\n",
    "$$\n",
    "Intuitively, this says: \"*there is a Large Red Circle*\". Here are some critical constraints:\n",
    "1. In each object clause, there can be at least 1 feature specification at most 3, and no\n",
    "   two specifications describe the same feature dimension. For example, you\n",
    "   cannot have $\\exists_x[\\text{Color(x) = Red} \\land \\text{Color(x) = Blue}]$.\n",
    "2. In each object clause, all feature specifications are connected through\n",
    "   conjunctions\n",
    "3. In each rule, there is exactly 1 object clause.\n",
    "\n",
    "Given this specification, there are $26$ different rules. Make sure you\n",
    "understand why they add up to $26$. To give you a hint, you should approach this\n",
    "problem by dividing the rules into three categories: rules with $1$, $2$, and\n",
    "$3$ feature specifications.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To help you with this task, I include a helper function to generate\n",
    "combinations. You might need this to generate the rule families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sets of size r from sample space of n elements\n",
    "def multiset_comb(n, r, constructor = tuple):\n",
    "\tindices = [0] * r\n",
    "\n",
    "\tyield constructor(indices)\n",
    "\twhile True:\n",
    "\t\t# find the right-most index that does not reach the end\n",
    "\t\tfor i in reversed(range(r)):\n",
    "\t\t\tif indices[i] != n - 1:\n",
    "\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\t# if all indices are n - 1, done\n",
    "\t\t\treturn\n",
    "\t\t# e.g. if n = 3, (0, 1, 2) --> (0, 2, 2)\n",
    "\t\tindices[i:] = [indices[i] + 1] * (r - i)\n",
    "\t\tyield constructor(indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiset_comb function generate *indexes* that you can use. The nature of\n",
    "this function should become clear to you with the following demonstration.\n",
    "Suppose that we want to generate all Type-Singular rules that have only one\n",
    "feature specification. To start, we know that each rule chooses one feature from\n",
    "the three. Using $n = 3$ and $r = 1$ we can generate the feature position indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(1,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "Feature_Positions = list(multiset_comb(3,1))\n",
    "for pos in Feature_Positions: print(pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `multiset_comb` function give us a sequence of indices to\n",
    "select. When we iterate through this sequence, we select the first, second, and\n",
    "third feature. \n",
    "\n",
    "You might notice that the output of `multiset_comb` is converted to a list. This\n",
    "is because the function gives a *generator* that does not store anything but\n",
    "generates results on the go. One certain advantage of generators over lists is\n",
    "that they take minimal memory. However, this means you cannot index a generator\n",
    "like you index a list, and you need to reset the generator once you finish a\n",
    "loop if you want to reuse it. Generators are useful when we need to generate a large\n",
    "group of things but only need to use a fraction of them. In our project this is\n",
    "not a big deal. You can read more about generators in Python's official\n",
    "documentation.\n",
    "\n",
    "We can map these indexes to select these features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Feature Selection: Color\n",
      "Current Feature Selection: Shape\n",
      "Current Feature Selection: Size\n"
     ]
    }
   ],
   "source": [
    "Feature_Names = [\"Color\", \"Shape\", \"Size\"]\n",
    "for pos in Feature_Positions:\n",
    "    feature_index = pos[0]\n",
    "    print(\"Current Feature Selection:\", Feature_Names[feature_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we know that each feature has two states. When a feature is selected,\n",
    "there are two states this feature specification can take. We will use\n",
    "`multiset_comb` in a similar manner to generate the two states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Feature State for Color: Red\n",
      "Current Feature State for Color: Blue\n"
     ]
    }
   ],
   "source": [
    "Features = [[\"Red\", \"Blue\"], [\"Circle\", \"Square\"], [\"Large\", \"Small\"]]\n",
    "Feature_States = list(multiset_comb(2,1))\n",
    "for st in Feature_States:\n",
    "    state_index = st[0]\n",
    "    print(\"Current Feature State for Color:\", Features[0][state_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to combine the two lists together. This is relatively simple,\n",
    "because the feature states and feature positions are independent. This means we\n",
    "can just get the Cartesian product of the two list to get all possible combination of\n",
    "positions and states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0,), (0,))\n",
      "((0,), (1,))\n",
      "((1,), (0,))\n",
      "((1,), (1,))\n",
      "((2,), (0,))\n",
      "((2,), (1,))\n"
     ]
    }
   ],
   "source": [
    "All_Rules = list(product(Feature_Positions, Feature_States))\n",
    "for rule in All_Rules:\n",
    "    print(rule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration give us a pair of indices, with the first and second indicating\n",
    "the position and state of the features. We can them map them to the actual\n",
    "features to form rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color : Red\n",
      "Color : Blue\n",
      "Shape : Circle\n",
      "Shape : Square\n",
      "Size : Large\n",
      "Size : Small\n"
     ]
    }
   ],
   "source": [
    "for rule in All_Rules:\n",
    "    pos = rule[0][0]\n",
    "    st = rule[1][0]\n",
    "    print(Feature_Names[pos],\":\",Features[pos][st])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give you all $6$ Type-Singular rules that have only $1$ feature\n",
    "specification. You can generalize this process to rules that have $2$ and $3$\n",
    "feature specifications. For example, when you use `multiset_comb` to generate\n",
    "the position of $2$ features, you will get a sequence of pairs of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "Two_Specs_Pos = list(multiset_comb(3,2))\n",
    "for pos_pair in Two_Specs_Pos: print(pos_pair)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to figure out a way to apply this same process for the rules with\n",
    "2 and 3 feature specifications. Specifically, your tasks are:\n",
    "1. **Develop a way to represent the rules.** Keep in mind that your rule\n",
    "   representation should be made in a way that it is easy to verify whether a\n",
    "   given object combination satisfy the rule (you should consider situations\n",
    "   where 3 objects show up together to account for the generalization stimuli). Also, you should keep your rule representation\n",
    "   to be uniform, so it's a good idea to read the next section about counting\n",
    "   conjunctions before deciding on a uniform representation of the rule\n",
    "   famileis.\n",
    "2. **Generate all the $26$ rules in the Type-Singular rule family**.\n",
    "3. **Select $3$ representative rules from this family and verify that they accept\n",
    "   and reject all $36$ stimuli correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c. Counting Conjuntion Rule Representation\n",
    "Generally speaking, a counting conjunction rule takes the following form:\n",
    "$$\n",
    "\\exists_o^{=2}[\\text{Color(o) = Red}] \\land \\exists_o^{=1}[\\text{Shape(o) =\n",
    "Circle}] \\land \\exists_o^{=0}[\\text{Size(o) = Large}]\n",
    "$$\n",
    "Intuitively, this says *there are two red things, exactly one circle, and no large thing*. Note that the counting conjunction rules *do not* care\n",
    "about the distribution of these features within the two objects. As long as the\n",
    "features of the two object added together satisfy the specifications, the\n",
    "stimulus satisfy the rule. Again, here are some constraints:\n",
    "1. There is at least $1$ feature specification and at most $6$.\n",
    "2. Each feature specification can count to $0$, $1$, and $2$\n",
    "3. Exclude the rules that has no positive extensions (e.g., rules that asks for\n",
    "   $2$ large things and $2$ small things)\n",
    "\n",
    "Note that the counting conjunctions only consider *exact* matches. When we only\n",
    "have two objects and when each feature only has binary states, a lot of these\n",
    "rules will be describing the same extension. For example, the rule that asks for\n",
    "two red things is the same as the rule that asks for no blue thing. For now, we\n",
    "will allow these repeatitions. \n",
    "\n",
    "This problem is easier to solve if we disregard **constraint\n",
    "3** and consider the other constraints first. One way to approach this\n",
    "problem is to extend our methods in **1b**, where we divide the rule families\n",
    "into three categories: rules that specify $1$, $2$, and $3$ features. Within the\n",
    "$1$ specification family, there are $45$ rules. This is because each\n",
    "state of the feature can have $4$ counting quantifications:\n",
    "$$\n",
    "[+0, =0, =1, =2]\n",
    "$$\n",
    "where $+0$ here encodes the quantification that does not care about this\n",
    "feature state. For example, the rule $\\exists o^{+0}[\\text{Color(o) = Red}]\n",
    "\\land \\exists o^{=2}[\\text{Color(o) = Blue}]$ only asks for $2$ blue things.\n",
    "$+0$ is not a standard notation and you can make this into whatever notation you\n",
    "want. We only include this here to make rule generation easier. Now, since each feature state has $4$ quantifications, each feature will have\n",
    "$4^2$ unique configurations, and $4^2 - 1 = 15$ configurations that are\n",
    "non-empty. As a result, the $1$ specification sub-family has $\\text{nCr}(3,1) \\times\n",
    "15 = 45$ rules. Following the same principle, the $2$ specification sub-family\n",
    "has $\\text{nCr}(3,2) \\times 15^2 = 675$ rules, and the $3$ sub-family has\n",
    "$\\text{nCr}(3,3) \\times 15^3 = 3375$ rules.  \n",
    "\n",
    "You can follow this method outlined above. However, there is also a much simpler\n",
    "way to solve this problem (hint: $45 + 675 + 3375 = 4095$). Free free to try\n",
    "this method instead.\n",
    "\n",
    "Once we have all the possible rules in the family, we can now consider\n",
    "**constraint 3**. Here we can take the easy way out through brute-force. That\n",
    "is, we simply check all the $4095$ rules with the $36$ stimuli, and we cast out\n",
    "the rules satisified by none of the stimuli. This will give us $999$ workable\n",
    "rules in the family.\n",
    "\n",
    "Hence, your tasks in this section are:\n",
    "1. **Generate all $4095$ rules**. You are free to choose your method.\n",
    "2. **Cast out impossible rules to obtain the $999$ rules**.\n",
    "3.  **Select $3$ representative rules from this family and verify that they accept\n",
    "   and reject all $36$ stimuli correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Section 2. Bayesian Modeling\n",
    "\n",
    "## 2a. Prior\n",
    "Now that we have set up the object and rule representation, we have all the\n",
    "required machinary to tackle the Bayesian model. Given a rule $h$ (hypothesis)\n",
    "in a family $\\mathcal{H}$ (hypothesis space), the uniform prior function is given\n",
    "by:\n",
    "$$\n",
    "p(h) = \\frac{1}{|\\mathcal{H}|}\n",
    "$$\n",
    "This is an uninformative prior. Typical Bayesian models usually employ a complex\n",
    "prior, either penalizing the length of the rule or defining exact probablity of\n",
    "production rules in a Context-Free Grammar that are used to generate the rules.\n",
    "However, the uniform prior is good enough for our purpose here. \n",
    "\n",
    "However, one trick your should learn to master is to cast these probabilities in *log*\n",
    "space. The reason behind this maneuver has to do with *floating point error*.\n",
    "The short story is that computers operate on discrete representations of the\n",
    "continuous number space, and these imprecisions add up with lengthy\n",
    "computations. Computation of probability is especially susceptible to this error\n",
    "since we are dealing with very small numbers. Casting probability computation\n",
    "into log space circumvent this problem to some degree and offer good enough\n",
    "precision.\n",
    "\n",
    "The basic idea you need to know is that, in log space, multiplications and\n",
    "divisions become addition and substraction. Beside `np.log` and `np.exp`, you\n",
    "probably need the `logsumexp` function that calculate the log of the sum of the\n",
    "exponenets of the input. This function is imported below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your tasks are:\n",
    "1. **Write a function to generate prior probability for the two rule families in\n",
    "   log space**.\n",
    "2. **Check that the log prior sums to 0**. You should do this by applying\n",
    "   `logsumexp` to your prior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Likelihood\n",
    "Given a stimuli at trial i $s_i$ and its outcome $\\mathcal{O}(s_i)$, we can obtain its\n",
    "likelihood conditioned on a specific rule through the following formula:\n",
    "$$\n",
    "p(s_i, \\mathcal{O}(s_i)|h \\in \\mathcal{H}) = \n",
    "    \\begin{cases}\n",
    "    1 - m  & h \\textrm{ matches } s_i, \\mathcal{O}(s_i) \\\\\n",
    "    m & \\textrm{otherwise} \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "where $0 < m < 0.5$ is the mismatch free parameter. You can understand $m$ as the\n",
    "uncertainty of the model: the higher $m$ is, the less certain is the model in\n",
    "the observation. Higher $m$ results in slower convergence, which means the model\n",
    "will take more trials to infer the ground truth. Here we will just fix $m$ to be\n",
    "$0.1$, but it is a good idea to make this a variable that you can change in the\n",
    "future (for **section 4**). Note that \"$h$ matches $s_i,\n",
    "\\mathcal{O}(s_i)$\" is the same as $s_i$ satisfying the rule, which you have\n",
    "implemented in **section 1**.\n",
    "\n",
    "Your task in this section is:\n",
    "1. **Implement the likelihood function for both families**. Remember to cast\n",
    "   these likelihoods into the log space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Marginal/Prediction\n",
    "The model makes a prediction about the outcome of a particular stimulus $s_i$ by\n",
    "generating the probability that the stimulus triggers the effect $p(\\mathcal{O}(s_i)\n",
    "= 1)$. To do this, we take the product of likelihood and prior of individual\n",
    "rules and sum them together:\n",
    "$$\n",
    "p(\\mathcal{O}(s_i) = 1) = \\sum_{h \\in \\mathcal{H}} p(s_i, \\mathcal{O}(s_i) =\n",
    "1|h) \\times p(h) \\\\\n",
    "$$\n",
    "Casted into log space, this becomes:\n",
    "$$\n",
    "\\text{log } p(\\mathcal{O}(s_i) = 1) = \\text{log } \\sum_{h \\in \\mathcal{H}} e^{\\text{log } p(s_i,\\mathcal{O}(s_i) =\n",
    "1|h) + \\text{ log }p(h)}\n",
    "$$\n",
    "Note that the right hand side of the equation is the `logsumexp` function\n",
    "applied to the sum of your likelihoods and priors arrays in log space.\n",
    "\n",
    "You task in this section is:\n",
    "1. **Implement the Marginal function**. Note that the output of this function\n",
    "   should still be in log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Posterior\n",
    "Now we have all the necessary component to obtain the posterior. The Bayes\n",
    "theorem shows that the posterior probability of a rule given the previous\n",
    "observations is:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathcal{h}|s_i, \\mathcal{O}(s_i) = 1) &= \\frac{p(s_i, \\mathcal{O}(s_i) =\n",
    "1|h) \\times p(h)}{p(s_i, \\mathcal{O}(s_i) = 1)} \\\\\n",
    "&= \\frac{\\text{likelihood}_h \\times \\text{prior}_h}{\\text{Marginal}_\\mathcal{H}}\n",
    "\\\\\n",
    "&= e^{\\text{log likelihood}_h + \\text{ log prior}_h - \\text{ log Marginal}_\\mathcal{H}}\n",
    "\\end{align}\n",
    "$$\n",
    "Of course, when you take away the exponent the last formula will give you a log\n",
    "posterior. By replacing this posterior with the prior, you update the model with\n",
    "the results according to the Bayes theorem.\n",
    "\n",
    "Your tasks in this section are:\n",
    "1. **Implement the posterior function**. Again, keep track of the space you are\n",
    "   in; you do not want to update your log prior with a natural posterior.\n",
    "2. **Run your Type-Singular and Counting Conjunction model through all $36$\n",
    "   unique stimuli and get the top $3$ rules at the end**. That is, after\n",
    "   updating the model with all $36$ unique stimuli, find the top $3$ rules with\n",
    "   the highest posterior. You should be able to get the $36$ stimuli from the G1\n",
    "   block of the subject's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Section 3. Evaluating Model Performances\n",
    "Now that you have working Bayesian modela, it's time to put them into tests. The\n",
    "basic idea is that you present the stimuli to the model in the same order they\n",
    "are shown to the subject. You should have loaded the experiment data in\n",
    "**section 1a**. Beside the `Seq` and `Blc` column, you should pay attention to\n",
    "the `Rsp` and `Truth` columns, which encode the subjects' response and the\n",
    "truth. `1` indicates the effect happens. Your task in this section is:\n",
    "1. **Update your model with the stimuli in `Blc` `1` and `2`.**\n",
    "2. **Freeze your model and obtain its prediction for the stimuli in `Blc`\n",
    "   `G1` and `G2`.** That is, you should not update the prior with posterior in\n",
    "   these two secionts. You should measure the models' performance in two ways (you should revert your models' prediction back to the\n",
    "\tnature space to do this.):\n",
    "\t- 2.1. **Measure the absoluate difference between Model's prediction and the\n",
    "\tground truth**\n",
    "\t- 2.2 **Measure the absoluate difference between Model's prediction and\n",
    "\tsubject's responses**\n",
    "3. **Plot the models' performances in bar plots.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Section 4. Fitting Models to Subjects\n",
    "The stock Bayesian model we have are doing Bayesian inferences, but they are too\n",
    "good to approximate subjects' learning processes. Model fitting is a huge\n",
    "problem in it's own, so we will only touch on the basics of basics here. Recall\n",
    "that, in **section 2b.**, we $m$ as a free parameter to control the uncertainty\n",
    "of the model. We will vary $m$ to obtain a better model fit. In addition, we\n",
    "will introduce two extra free parameters to further fit the subjects. Recall\n",
    "that, in **section 2c.** we defined the marginal as:\n",
    "$$\n",
    "p(\\mathcal{O}(s_i) = 1) = \\sum_{h \\in \\mathcal{H}} p(s_i, \\mathcal{O}(s_i) =\n",
    "1|h) \\times p(h) \\\\\n",
    "$$\n",
    "we will add two extra free parameters, $\\alpha, \\beta$ so that: \n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathcal{O}(s_i) = 1) = & \\;\\; \\alpha \\times [\\sum_{h \\in \\mathcal{H}} p(s_i, \\mathcal{O}(s_i) =\n",
    "1|h) \\times p(h)] \\\\\n",
    "& + (1 - \\alpha) \\times \\beta\n",
    "\\end{align}\n",
    "$$\n",
    "where $0 \\leq \\alpha \\leq 1$ represents the subjects' tendency to make\n",
    "predictions according to their own hypotheses and $0 \\leq \\beta \\leq 1$ represents\n",
    "the subjects' tendency toward responding positively. That is, in $\\alpha$ of trials\n",
    "subjects respond as an ideal bayesian learner, and in $(1 - \\alpha)$ of trials\n",
    "subjects respond randomly. Importantly, **do not use this Marginal to update the\n",
    "posterior** (although I am on the fence about whether this is the correct way of\n",
    "doing things, let's keep things this way for now). In each trial, you should collect two set of data: the posterior\n",
    "probability obtained in **section 2c.**, and the model's fitted prediction\n",
    "obtained in this section. While you use the latter to generate the model's\n",
    "prediction, you use the former to do Bayesian updates.\n",
    "\n",
    "Now we have three free parameters: $m, \\alpha, \\beta$. While $m$ varies between\n",
    "$0.5$ and $1$, $\\alpha$ and $\\beta$ varies between $0$ and $1$. We are going to\n",
    "fit the subject's data using grid search. We should test all possible values for\n",
    "each parameters with a $0.1$ spacing. Including the two extremes, this will give\n",
    "us $6$ values for $m$ and $11$ values for $\\alpha$ and $\\beta$. This results in\n",
    "$6 \\times 11 \\times 11 = 726$ combinations of values for the three parameters. \n",
    "\n",
    "Your tasks in this section are:\n",
    "1. **Test all $726$ value combinations of the three free parameter for each\n",
    "   model and get the best combination for each model**. The performance is\n",
    "   measured in the absolute difference between models' and subject's predictions\n",
    "   introduced in **section 3**.\n",
    "2. **Reproduce the performance bar plots of the best models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
